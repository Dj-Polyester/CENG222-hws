\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{amsmath}
\usepackage{hyperref}

\usepackage[hmargin=3cm,vmargin=6.0cm]{geometry}
\topmargin=-2cm
\addtolength{\textheight}{6.5cm}
\addtolength{\textwidth}{2.0cm}
\setlength{\oddsidemargin}{0.0cm}
\setlength{\evensidemargin}{0.0cm}
\usepackage{indentfirst}
\usepackage{amsfonts}

\begin{document}

\section*{Student Information}

Name : Batuhan Karaca \\

ID : 2310191 \\


\section*{Answer 1}
\subsection*{a)}
We are given the information that the selected box is $X$. 
This means we could only choose a ball from $X$. 
This reduces, or restricts the sample space to the set 
of the balls in $X$. There are $2$ \textit{green} 
balls in $X$, $6$ balls in total. \hypertarget{method}{Then}
\\ \\
\begin{tabular}{l l}
    & $|\Omega_X|$: Total number of balls in $X$ (reduced sample space) \\ \\
    & $|green_X|$: Total number of \textit{green} balls in $X$ \\ \\
    & $P(green\ |\ X)=\frac{|green_X|}{|\Omega_X|}=\frac{2}{6}=\frac{1}{3}\sim \underline{\underline{0.33}}$
\end{tabular}
\subsection*{b)}
In the textbook page 30, \textit{\textbf{Law of Total Probability}} is given
\\ \\
\begin{tabular}{l l l}
    && \textit{Consider some partition of the sample space $\Omega$ with mutually exclusive and exhaustive}\\
    &&\textit{events $B_1 , . . . , B_k$ . It means that }\\
    \\
    && $B_i \cap B_j = \emptyset$ for any $i \neq j$ and $B 1 \cup . . . \cup B_k = \Omega.$\\
    \\
    &&\textit{These events also partition the event $A$,}\\
    \\
    && $A=(A \cap B_1 )\cup . . . \cup(A \cap B_k ),$\\
    \\
    && \textit{and this is also a union of mutually exclusive events [...]. Hence,}\\
    \\
    && $P(A) = \sum\limits_{j=1}^{k} P\{A \cap B\}$\\
\end{tabular}
\\ \\
Picking a \textit{red} ball from either $X$, or $Y$ are disjoint events 
, since set of \textit{red}
balls in $X$ and $Y$ are disjoint.
In this case, $A$ corresponds to the set of \textit{red} balls, denoted as $red$.
By \hypertarget{eq1}{\textit{\textbf{Law of Total Probability}}}
\\ \\
\begin{tabular}{l l}
    & $P(red) = P\{red \cap X\} + P\{red \cap Y\}\quad (1)$\\
\end{tabular}
\\ \\
In the textbook page 27, \hypertarget{cond}{formula of the \textit{conditional probability}} is given
\\ \\
\begin{tabular}{l l}
    & $P\{A\ |\ B\}=\frac{P\{A\cap B\}}{P(B)} $\\
    & $P\{A\cap B\}=P(A|B)*P(B) $\\
\end{tabular}
\\ \\
Rewriting the expression \hyperlink{eq1}{$(1)$}, in terms of \textit{conditional probabilities},
\\ \\
\begin{tabular}{l l}
    & $P(red) = P\{red\ |\ X\}*P(X) + P\{red\ |\ Y\}*P(Y)$\\
\end{tabular}
\\ \\
Using the \hyperlink{method}{methodology} in \textbf{part a}, we can calculate the \textit{conditional probabilities} as
$P\{red\ |\ X\}=1/3\sim 0.3$,$P\{red\ |\ Y\}=1/5=0.2$. We know $P(X)=0.4$.
Choosing either $X$ or $Y$, are exhaustive and disjoint events. Therefore, $P(X)+P(Y)=1$,
hence $P(Y)=0.6$. Substituting gives $P(red)\sim\underline{\underline{0.25}}$

\subsection*{c)}
By \textit{\textbf{Bayes' Rule}}
\\ \\
\begin{tabular}{l l}
    & $P(Y\ |\ blue)=\frac{P(blue\ |\ Y)*P(Y)}{P(blue)}$\\
\end{tabular}
\\ \\
Using the \hyperlink{method}{methodology} in \textbf{part a}, we can calculate $P\{blue|Y\}=2/5=0.4$. . We know
$P(Y)=0.6$ from \textbf{part b}. Furthermore, using \textit{\textbf{Law of Total Probability}}, as done in \textbf{part b},
we can calculate $P(blue) = P\{blue | X\}*P(X) + P\{blue | Y\}*P(Y) = (1/3)(0.4)+(0.4)(0.6)\sim0.37$. Then
$P(Y|blue)\sim \underline{\underline{0.64}}$
\section*{Answer 2}
Sample space will be denoted $\Omega$. 
\textit{\textbf{Mutually exhaustive}} events are also called \textit{\textbf{disjoint}} events.
\\ \\
\begin{tabular}{l l}
    & Events $A_1, A_2 ... A_k$ are disjoint \underline{if and only if} the sets $A_i \cap A_j = \emptyset$ for each and every pair \\
    & Events $A_1, A_2 ... A_k$ are exhaustive \underline{if and only if} the sets $A_1 \cup A_2 \cup... \cup A_k=\Omega$  \\
\end{tabular}
\\ \\
\subsection*{a)}
By de Morgan's laws we have $\overline{A} \cup \overline{B} = \overline{(A \cap B)}$.
We also know $\overline{\emptyset}=\Omega$ and $\overline{\overline{\emptyset}}=\emptyset=\overline{\Omega}$.
\\ \\
\textbf{1)}If $A$ and $B$ are disjoint, then
\\ \\
\begin{tabular}{l l}
    &$A \cap B = \emptyset$\\
    &$\overline{A} \cup \overline{B} = \overline{(A \cap B)} = \overline{\emptyset}=\Omega$\\
\end{tabular}
\\ \\
The sets $\overline{A}$ and $\overline{B}$ are exhaustive.
\\ \\
\textbf{2)}If $\overline{A}$ and $\overline{B}$ are exhaustive, then
\\ \\
\begin{tabular}{l l}
    &$\overline{A} \cup \overline{B} = \Omega$\\
    &$\overline{(A \cap B)}=\overline{A} \cup \overline{B} = \Omega$\\
    &$A \cap B = \overline{(\overline{A} \cup \overline{B})} = \emptyset$\\
\end{tabular}
\\ \\
the sets $A$ and $B$ are disjoint completing the proof.
\subsection*{b)}
By de Morgan's laws we have $\overline{A} \cup \overline{B} \cup \overline{C} = \overline{(A \cap B \cap C)}$.
We also know $\overline{\emptyset}=\Omega$ and $\overline{\overline{\emptyset}}=\emptyset=\overline{\Omega}$.
\\ \\
\textbf{1)}If $A$,$B$ and $C$ are disjoint, then
\\ \\
\begin{tabular}{l l}
    &$A \cap B \cap C = \emptyset$\\
    &$\overline{A} \cup \overline{B} \cup \overline{C} = \overline{(A \cap B \cap C)} = \overline{\emptyset}=\Omega$\\
\end{tabular}
\\ \\
The sets $\overline{A}$,$\overline{B}$ and $\overline{C}$ are exhaustive.
\\ \\
\textbf{2)}If $\overline{A}$,$\overline{B}$ and $\overline{C}$ are exhaustive, then
\\ \\
\begin{tabular}{l l}
    &$\overline{A} \cup \overline{B} \cup \overline{C}= \Omega$\\
    &$\overline{(A \cap B \cap C)}=\overline{A} \cup \overline{B} \cup \overline{C} = \Omega$\\
    &$A \cap B \cap C = \overline{(\overline{A} \cup \overline{B} \cup \overline{C})} = \emptyset$\\
\end{tabular}
\\ \\
the sets $A$, $B$ and $C$ are disjoint completing the proof.
\section*{Answer 3}

\subsection*{a)}
In the textbook page 58, formula of the \textit{Binomial probability mass function} is given as
\\ \\
\begin{tabular}{l l}
    & $P (x) = P \{X = x\} = \binom nx p^xq^{n-x}$\\
\end{tabular}
\\ \\
\textit{which is the probability of exactly x successes in n trials.} In this example,
there are $5$ dice thrown, each die is being a \textit{\textbf{Bernoulli trial}}. This 
is a sequence of \textbf{Bernoulli trials}. Therefore, we can use \textit{\textbf{Binomial Distribution}}.
In this case, the probability of success $p$ is ,$2$ outcomes ($5$ and $6$) out of $6$ outcomes in total, $\frac{1}{3}$.
$q$ is the complement of $p$, as both events(getting success and failure) are disjoint and exhaustive. $q=1-p=\frac{2}{3}$. 
There are $5$ trials ($n=5$) and exactly $2$ successful outcomes ($x=2$). Subsituting gives
\\ \\
\begin{tabular}{l l}
    & $P (x) = P \{X = x\} = \binom 52 (\frac{1}{3})^2(\frac{2}{3})^3 \sim \underline{\underline{0.33}}$\\
\end{tabular}
\\ \\
\subsection*{b)}
In the textbook page 41, formula of the \textit{\textbf{Cumulative Distribution Function (cdf) }} is given
\\ \\
\begin{tabular}{l l}
    & $F(x)=P\{ X \leq x \}=\sum\limits_{y \leq x} P(y)$\\
\end{tabular}
\\ \\
Probability of having exactly $k$ succesful dice is $P(k)$, 
having less than,or equal to $k$ succesful dice is $P\{X \leq k \}=\sum\limits_{y \leq k} P(y)$,
having at least $k$ succesful dice is $P\{X \geq k \}=\sum\limits_{y \geq k} P(y)$.
Since for exhaustive events, probability sum of the events in the sample space is
$\sum\limits_{y} P(y)=1$, we have 
\\ \\
\begin{tabular}{l l}
    & $P\{X \geq k \}=\sum\limits_{y \geq k} P(y)$\\
    & \\
    & $P\{X \geq k \}=\sum\limits_{y} P(y)-\sum\limits_{y \leq k-1} P(y)$\\
    & \\
    & $P\{X \geq k \}=1-\sum\limits_{y \leq k-1} P(y)$\\
    & \\
    & $P\{X \geq k \}=1-F(k-1)$\\
\end{tabular}
\\ \\
By the definition of \textit{cdf}. Substituting gives
\\ \\
\begin{tabular}{l l}
    & $P\{X \geq 2 \}=1-F(1)=1-\sum\limits_{y \leq 1} P(y)=1-(P(0)+P(1))$\\
\end{tabular}
\\ \\
$P(0)= \binom 50 (\frac{1}{3})^0(\frac{2}{3})^{5} \sim 0.13$ and $P(1)= \binom 51 (\frac{1}{3})^1(\frac{2}{3})^{4} \sim 0.33$ as
they can be calculated by using the \textit{Binomial Distribution}.
\\ \\
\begin{tabular}{l l}
    & $P\{X \geq 2 \}=1-(P(0)+P(1))=1-((\frac{1}{3})^0(\frac{2}{3})^{6}+\binom 61 (\frac{1}{3})^1(\frac{2}{3})^{5}) \sim \underline{\underline{0.54}}$\\
\end{tabular}
\\ \\
is the answer.
\section*{Answer 4}
\subsection*{a)}
In the textbook page 45, formula of the \hypertarget{addrule}{\textit{Addition Rule}} is given
\\ \\
\begin{tabular}{l l}
    & $P_X(x)=P\{X=x\}=\sum\limits_y P_{(X,Y)} (x,y)$\\
    & $P_Y(y)=P\{Y=y\}=\sum\limits_x P_{(X,Y)} (x,y)$\\
\end{tabular}
\\ \\
In this case, we have $3$ variables. \hypertarget{eq2}{Similarly},
\\ \\
\begin{tabular}{l l}
    & $P\{X=x,Y=y\}=\sum\limits_z P_{(X,Y,Z)} (x,y,z)\quad (2)$\\
\end{tabular}
\\ \\
For any random variable $X, Y$ and $Z$, numbers $x$ and $y$. Then
\\ \\
\begin{tabular}{l l}
    & $P\{A=1,C=0\}=\sum\limits_b P_{(A,B,C)} (1,b,0)=P_{(A,B,C)} (1,0,0)+P_{(A,B,C)} (1,1,0)=0.06+0.09=\underline{\underline{0.15}}$\\
\end{tabular}
\\ \\
\subsection*{b)}
Similar to the previus part (\textit{part a}), however in this case, $2$ variables are unknown in $3$ variables
\\ \\
\begin{tabular}{l l}
    & $P\{X=x\}=\sum\limits_y\sum\limits_z P_{(X,Y,Z)} (x,y,z)$\\
\end{tabular}
\\ \\
We have $B=1$
\\ \\
\begin{tabular}{l l l}
    & $P\{B=1\}$ & $=\sum\limits_a\sum\limits_c P_{(A,B,C)} (a,1,c)$\\
    & & \\
    & & $=\sum\limits_a (\ P_{(A,B,C)} (a,1,0)+P_{(A,B,C)} (a,1,1)\ )$\\
    & & \\
    & & $=P_{(A,B,C)} (0,1,0)+P_{(A,B,C)} (1,1,0)+P_{(A,B,C)} (0,1,1)+P_{(A,B,C)} (1,1,1)$\\
    & & \\
    & & $=0.21+0.09+0.02+0.08$\\
    & & \\
    & & $=0.4$\\
\end{tabular}
\\ \\
\subsection*{c)}
In the textbook page 45, \textit{\textbf{Independence of Random Variables}} is defined
\\ \\
\begin{tabular}{l l}
    & \textit{Random variables $X$ and $Y$ are independent if}\\
    &$\quad P_{(X,Y)}(x,y)=P_X(x)P_Y(y)$\\
\end{tabular}
\pagebreak
\\ \\
\textit{Marginal Distribution} of $A$ and $B$
\\ \\
\begin{tabular}{l l l}
    & $P_A(a)=P\{A=a\}$&$=\sum\limits_b\sum\limits_c P_{(A,B,C)} (a,b,c)$\\
    &&\\
    & $P_A(0)=P\{A=0\}$&$=\sum\limits_b\sum\limits_c P_{(A,B,C)} (0,b,c)$\\
    &&\\
    & &$=\sum\limits_b (\ P_{(A,B,C)} (0,b,0)+P_{(A,B,C)} (0,b,1)\ )$\\
    &&\\
    & &$=P_{(A,B,C)} (0,0,0)+P_{(A,B,C)} (0,1,0)+P_{(A,B,C)} (0,0,1)+P_{(A,B,C)} (0,1,1)$\\
    &&\\
    & &$=0.14+0.21+0.08+0.02$\\
    &&\\
    & &$=0.45$\\
    &&\\
    & $P_B(b)=P\{B=b\}$&$=\sum\limits_a\sum\limits_c P_{(A,B,C)} (a,b,c)$\\
    &&\\
    & $P_B(1)=P\{B=1\}$&$=\sum\limits_a\sum\limits_c P_{(A,B,C)} (a,1,c)$\\
    &&\\
    & &$=\sum\limits_a (\ P_{(A,B,C)} (a,1,0)+P_{(A,B,C)} (a,1,1)\ )$\\
    &&\\
    & &$=P_{(A,B,C)} (0,1,0)+P_{(A,B,C)} (1,1,0)+P_{(A,B,C)} (0,1,1)+P_{(A,B,C)} (1,1,1)$\\
    &&\\
    & &$=0.21+0.09+0.02+0.08$\\
    &&\\
    & &$=0.40$\\
    &&\\
\end{tabular}
\\ \\
Using \hyperlink{eq2}{$(2)$} in \textit{\textbf{part a}},
\\ \\
\begin{tabular}{l l l}
    & $P_{(A,B)}(0,1)=P\{A=0,B=1\}$&$=\sum\limits_c P_{(A,B,C)} (0,1,c)$\\
    &&\\
    &&$=P_{(A,B,C)} (0,1,0)+P_{(A,B,C)} (0,1,1)$\\
    &&\\
    &&$=0.21+0.02$\\
    &&\\
    &&$=0.23$\\
\end{tabular}
\\ \\
We see that $P_{(A,B)}(0,1)\neq P_A(0)*P_B(1)$, which contradicts the \textit{\textbf{Independence of Random Variables}}
argument. Hence by contradiction, $A$ and $B$ \underline{\underline{are not independent}}.
\subsection*{d)}
Definition of the \textit{\textbf{Conditional Independence}} (from Wikipedia, refer \href{https://en.wikipedia.org/wiki/Conditional_independence#Definition}{here})
\\ \\
\begin{tabular}{l l}
    & \textit{In the standard notation of probability theory, $A$ and $B$ are conditionally independent given $C$}\\
    & \textit{ if and only if }\\
    &$\quad P(A \cap B\ |\ C) = P(A\ |\ C)P(B\ |\ C)$\\
\end{tabular}
\\ \\
Expanding both sides seperately, Using the \hyperlink{cond}{formula of \textit{conditional probability}}
\\ \\
\begin{tabular}{l l}
    &$P(A \cap B\ |\ C)= \frac{P((A \cap B) \cap C)}{P(C)}$\\
    &\\
    &$P(A\ |\ C)= \frac{P(A \cap C)}{P(C)}$\\
    &\\
    &$P(B\ |\ C)= \frac{P(B \cap C)}{P(C)}$\\
\end{tabular}
\\ \\
Then, $A$ and $B$ are conditionally independent given $C$ if and only if
\\ \\
\begin{tabular}{l l}
    &$P((A \cap B) \cap C)=\frac{P(A \cap C)P(B \cap C)}{P(C)}$\\
\end{tabular}
\\ \\
In this case, $C=1$ given
\\ \\
\begin{tabular}{l l}
    &$P\{(A \cap B) \cap (C=1)\}=\frac{P\{A \cap (C=1)\}P\{B \cap (C=1)\}}{P\{C=1\}}$\\
\end{tabular}
\\ \\
By the \hyperlink{addrule}{\textit{Addition Rule}}
\\ \\
\begin{tabular}{l l l}
    &$P\{C=1\}$&$=\sum\limits_a \sum\limits_b P_{(A,B,C)} (a,b,1)$\\
    &&\\
    &&$=\sum\limits_a P_{(A,B,C)} (a,0,1)+P_{(A,B,C)} (a,1,1)$\\
    &&\\
    &&$=P_{(A,B,C)} (0,0,1)+P_{(A,B,C)} (1,0,1)+P_{(A,B,C)} (0,1,1)+P_{(A,B,C)} (1,1,1)$\\
    &&\\
    &&$=0.08+0.32+0.02+0.08$\\
    &&\\
    &&$=0.50$\\
\end{tabular}
\\ \\
\begin{tabular}{l l}
    &$P\{(A \cap B) \cap (C=1)\}=\frac{P\{A \cap (C=1)\}P\{B \cap (C=1)\}}{0.50}$\\
\end{tabular}
\\ \\
Since we have a joint distribution, we can alter the \hypertarget{eq3}{expression}
\\ \\
\begin{tabular}{l l l}
    &$P_{(A,B,C)}(a,b,1)=\frac{P_{(A,C)}(a,1)P_{(B,C)}(b,1)}{0.50} \quad (3)$\\
\end{tabular}
\\ \\
For all $a,b \in \{0,1\}$. Using the \hyperlink{addrule}{\textit{Addition Rule}}
\\ \\
\begin{tabular}{l l}
    & $P_{(A,C)}(0,1)=0.08+0.02=0.10$\\
    & $P_{(A,C)}(1,1)=0.32+0.08=0.40$\\
    & $P_{(B,C)}(0,1)=0.08+0.32=0.40$\\
    & $P_{(B,C)}(1,1)=0.02+0.08=0.10$\\
\end{tabular}
\\ \\
Left hand sides (corresponding values from the table in the question)
\\ \\
\begin{tabular}{l l}
    & $P_{(A,B,C)}(0,0,1)=0.08$\\
    & $P_{(A,B,C)}(0,1,1)=0.02$\\
    & $P_{(A,B,C)}(1,0,1)=0.32$\\
    & $P_{(A,B,C)}(1,1,1)=0.08$\\
\end{tabular}
\\ \\
Right hand sides
\\ \\
\begin{tabular}{l l}
    & $\frac{P_{(A,C)}(0,1)P_{(B,C)}(0,1)}{0.50}=0.08$\\
    & \\
    & $\frac{P_{(A,C)}(0,1)P_{(B,C)}(1,1)}{0.50}=0.02$\\
    & \\
    & $\frac{P_{(A,C)}(1,1)P_{(B,C)}(0,1)}{0.50}=0.32$\\
    & \\
    & $\frac{P_{(A,C)}(1,1)P_{(B,C)}(1,1)}{0.50}=0.08$\\
\end{tabular}
\\ \\
Checking the equality\hyperlink{eq3}{$(3)$}
\\ \\
\begin{tabular}{l l}
    & $P_{(A,B,C)}(0,0,1)=\frac{P_{(A,C)}(0,1)P_{(B,C)}(0,1)}{0.50}=0.08$\\
    &\\
    & $P_{(A,B,C)}(0,1,1)=\frac{P_{(A,C)}(0,1)P_{(B,C)}(1,1)}{0.50}=0.02$\\
    &\\
    & $P_{(A,B,C)}(1,0,1)=\frac{P_{(A,C)}(1,1)P_{(B,C)}(0,1)}{0.50}=0.32$\\
    &\\
    & $P_{(A,B,C)}(1,1,1)=\frac{P_{(A,C)}(1,1)P_{(B,C)}(1,1)}{0.50}=0.08$\\
\end{tabular}
\\ \\
As seen, both sides are equal for all permutations of $a$ and $b$. The equality \hyperlink{eq3}{$(3)$} holds for all $a, b \in \{0,1\}$. Therefore 
given $C=1$, $A$ and $B$ are \underline{\underline{conditionally independent}}.
\end{document}

